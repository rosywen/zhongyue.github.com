{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering is one of the most used types of recommendation systems. It is also known as the Nearest Neighborhood algorithm. Collaborative filtering produces recommendations based on the knowledge of users’ preference for items. Stochastic gradient descent (SGD) is widely used for collaborative filtering. It is an approach that randomly picks one data point from the whole dataset at each iteration and gradually travels down by updating the parameters until it reaches the lowest point of that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, I load the main dataset \"user_artist_data.txt\", and then replace the \"goodid\" with \"badid\" using the information in \"artist_alias.txt\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "spark = SparkSession.builder.appName('recommend').master('local').getOrCreate()\n",
    "user_artist_df = spark.read.text('./profiledata_06-May-2005/user_artist_data.txt')\n",
    "artist_alias_df = spark.read.text('./profiledata_06-May-2005/artist_alias.txt')       \n",
    "user_artist = user_artist_df.withColumn('fields', split(col('value'), ' ')).select(\n",
    "            col('fields').getItem(0).alias('user_id').cast('int'),\n",
    "            col('fields').getItem(1).alias('artist_id').cast('int'),\n",
    "            col('fields').getItem(2).alias('play_count').cast('int')\n",
    "        ).drop('fields')\n",
    "artist_alias = artist_alias_df.withColumn('fields', split(col('value'), '\\t')).select(\n",
    "            col('fields').getItem(0).alias('artist_id').cast('int'),\n",
    "            col('fields').getItem(1).alias('goodid').cast('int')\n",
    "        ).drop('fields')\n",
    "\n",
    "#replace bad id to good id        \n",
    "df = user_artist.join(artist_alias, ['artist_id'], 'left')\n",
    "train_df = df.selectExpr('user_id', 'ifnull(goodid, artist_id) as artist_id', 'play_count')   \n",
    "#train_df = train_df.limit(5000) #first 5000 data (for training only)  \n",
    "\n",
    "#aggregate the views on the same user on the same artist\n",
    "train_df = train_df.groupBy(['user_id', 'artist_id']).agg({'play_count': 'sum'}).withColumnRenamed('sum(play_count)', 'play_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The play count of an artist is an implicit rating, which is the measurement that does not give direct feedback from users to show how much they like an item. People may have a different habit when listening to music, some people tend to repeat one song all the time, others may like to shuffle play even they like the artist. To avoid this, I add a new column \"score\", which scale the \"play count\" column to the range of 0-1 by dividing each play count by every user's maximum play number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardized play_count\n",
    "@udf(FloatType())\n",
    "def scaler_score(count, max_count):\n",
    "    return count/max_count\n",
    "\n",
    "max_train_df = train_df.groupBy('user_id').agg({'play_count': 'max'}).withColumnRenamed('max(play_count)', 'max_count')\n",
    "recommend_train_df = train_df.join(max_train_df, ['user_id'], 'left').withColumn('score', scaler_score('play_count', 'max_count')) \\\n",
    "    .select('user_id', 'artist_id', 'play_count', 'score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an n × m matrix of the score, n represents the number of users and m indicates the number of artists. In order to predict the rating $r_{ij}$ if target user i never listen to artist j before, I need to calculate the similarities between target user i and all other users, then make an artists recommendation for a target user by referring to the artist preference of top n most similar users.\n",
    "\n",
    "In class \"Recommend\", \"fit\" function trains the model to get the similarity between users. It first gathers the artists and the corresponding scores that related to the same user into the format like ('user_id', \\[(artist1,score1), (artist2,score2), ...\\]) and implements the cartesian product. Then the function calculates users' similarity between every two users by applying cosine similarity to their scores of common artists. Cosine similarity is calculated as $cos(x_1, x_2) = \\frac{x_1*x_2}{|x_1|*|x_2|}$. The greater the cosine value, the more the match between two vectors, 0 means there is no match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommend:\n",
    "    '''\n",
    "    Recommendation algorithm\n",
    "    '''\n",
    "    def __init__(self, uc='user_id', ic='item_id', sc='score', min_common_item=1):\n",
    "        '''\n",
    "        @param uc: column name of user id，default is \"user_id\"\n",
    "        @param ic: column name of artist id，default is \"item_id\"\n",
    "        @param sc: column name of score，default is \"score\"\n",
    "        @param min_common_item: the minimum number of common artists between two users，defualt value is 1\n",
    "        '''\n",
    "        self.uc = uc\n",
    "        self.ic = ic\n",
    "        self.sc = sc\n",
    "        self.min_common_item = min_common_item #only calculate similarity when there are common artists between two user \n",
    "        self.__sim_df = '' #similarity matrix\n",
    "    \n",
    "    def fit(self, train_df):\n",
    "        '''\n",
    "        Train the model to get similarity between users\n",
    "        '''\n",
    "        rating_by_user_rdd = train_df.rdd.map(lambda x: (x[self.uc], (x[self.ic], x[self.sc]))).groupByKey().mapValues(list)\n",
    "        rating_cross = rating_by_user_rdd.cartesian(rating_by_user_rdd)#cartesian product\n",
    "        user_sim_rdd = rating_cross.map(self.__user_sim).filter(lambda x: x is not None).groupByKey().mapValues(list) #calculate consine similarity between two user \n",
    "        schema = StructType([StructField(\"uid\", IntegerType(), True), StructField(\"user_id\", IntegerType(), True), \\\n",
    "                     StructField(\"sim\", FloatType(), True)])\n",
    "        sim_df = user_sim_rdd.flatMapValues(lambda x: x).map(lambda x:(x[0], x[1][0], float(x[1][1]))) \\\n",
    "            .toDF(schema=schema)\n",
    "        self.__sim_df = sim_df\n",
    "\n",
    "    def __user_sim(self, xi):\n",
    "        '''\n",
    "        Calculate users' similarity\n",
    "        '''\n",
    "        l1 = xi[0][1] #artist and score by user 1\n",
    "        l2 = xi[1][1] #artist and score by user 2\n",
    "        common_item = set([kv[0] for kv in l1]).intersection(set([kv[0] for kv in l2])) #same artists between two users\n",
    "        if len(common_item) >= self.min_common_item: #if have commmon artist\n",
    "            vector_1 = [kv[1] for kv in l1 if kv[0] in common_item] #scores of common artist\n",
    "            vector_2 = [kv[1] for kv in l2 if kv[0] in common_item]\n",
    "            cos = np.around(self.__cos_sim(vector_1, vector_2), 5) #cosine similarity\n",
    "            return (xi[0][0],  (xi[1][0], cos))\n",
    "\n",
    "    def __cos_sim(self, vector_a, vector_b):\n",
    "        '''\n",
    "        Calculate the cosine similarity\n",
    "        @param vector_a: user 1 vector (including all artists and corresponding scores related to a user)\n",
    "        @param vector_b: user 2 vector \n",
    "        '''\n",
    "        vector_a = np.mat(vector_a) \n",
    "        vector_b = np.mat(vector_b)\n",
    "        num = float(vector_a * vector_b.T)\n",
    "        denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "        cos = num / denom\n",
    "        return cos\n",
    "    \n",
    "    def get_sim_n(self, user_id, n): \n",
    "        '''\n",
    "        Get the most similiar n user to user_id \n",
    "        '''\n",
    "        sdg_sim = self.__sim_df.where('uid=\"'+user_id+'\"').orderBy(desc('sim')).limit(n)\n",
    "        return sdg_sim\n",
    "    \n",
    "    def get_sim_df(self):\n",
    "        return self.__sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_model = Recommend(uc='user_id', ic='artist_id', sc='score')\n",
    "recommend_model.fit(recommend_train_df)#get similarity between user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I implement the SGD algorithm in function \"fit_sgd\" below. First, I initialized all the biases value of users and artists to zeros. Then I apply the gradient descent method to each partition of data to update the biases value. Bias value, in this case, indicates the parameter we want to optimize using the SGD algorithm. Biases value are updated using $\\theta  \\leftarrow \\theta - \\alpha \\frac{\\partial f_k}{\\partial \\theta}$. At last, refer to the Parallelized Stochastic Gradient Descent algorithm [http://papers.nips.cc/paper/4006-parallelized-stochastic-gradient-descent.pdf], I calculate an averaged bias value after applying SGD within every partition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(df, uc='user_id', ic='artist_id'):\n",
    "    '''\n",
    "    initialise the bias of users and artists to 0 and store them in dictionary\n",
    "    '''\n",
    "    bu_df = df.select(uc).dropDuplicates().withColumn('bu', lit(0)) #add a constant conlumn\n",
    "    bi_df = df.select(ic).dropDuplicates().withColumn('bi', lit(0))\n",
    "    bu = bu_df.toPandas().set_index(uc).T.to_dict('int') #{userid :inial bias}\n",
    "    bi = bi_df.toPandas().set_index(ic).T.to_dict('int') #{artistid :inial bias}\n",
    "    return bu_df, bi_df, bu, bi\n",
    "    \n",
    "def get_gmean(df, sc='play_count'):\n",
    "    '''\n",
    "    Calculate the mean\n",
    "    '''\n",
    "    global_mean = df.select(mean(sc)).collect()[0][0]\n",
    "    return global_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sgd(train_df, num_partition, num_iter=500, spark=spark, bu={}, bi={}, uc='user_id', \n",
    "        ic='artist_id', sc='play_count', alpha=0.23, rate_alpha=0.99, reg=0.3):\n",
    "    '''\n",
    "    SGD implementation\n",
    "    @param train_df: train dataset\n",
    "    @param num_partition: number of partition\n",
    "    @param num_iter: number of iteration, default valuue is 500\n",
    "    @param spark: SparkSession\n",
    "    @param bu: users bias，default is empty dictionary\n",
    "    @param bi: artist bias，default is empty dictionary\n",
    "    @param uc: column name of user id，default is \"user_id\"\n",
    "    @param ic: column name of user id，default is \"item_id\"\n",
    "    @param sc: column name of user id，default is \"score\"   \n",
    "    @param alpha: learning rate，default value is 0.23\n",
    "    @param rate_alpha: changing rate of learning rate，default value is 0.99\n",
    "    @param reg: L2 regularization factor，default value is 0.3\n",
    "    '''\n",
    "    prec = 'predict'\n",
    "    bu_df, bi_df, bu, bi = init_bias(train_df) #inialised bias of userd and artists\n",
    "    global_mean = get_gmean(train_df) #mean of playcount\n",
    "    train_rdd = train_df.rdd\n",
    "    train_rdd = train_rdd.repartition(num_partition) #data repartition\n",
    "    #print('partition num: ', num_partition)\n",
    "    \n",
    "    #broadcast\n",
    "    bu_c = spark.sparkContext.broadcast(bu)\n",
    "    bi_c = spark.sparkContext.broadcast(bi)   \n",
    "    global_mean_c = spark.sparkContext.broadcast(global_mean)\n",
    "    uc_c = spark.sparkContext.broadcast(uc)\n",
    "    ic_c = spark.sparkContext.broadcast(ic)\n",
    "    sc_c = spark.sparkContext.broadcast(sc)\n",
    "    alpha_c = spark.sparkContext.broadcast(alpha)\n",
    "    reg_c = spark.sparkContext.broadcast(reg)\n",
    "     \n",
    "    def __calculate(iterator):\n",
    "        '''\n",
    "        calcualte gradient decent(update bias value) \n",
    "        '''\n",
    "        bu_v = bu_c.value\n",
    "        bi_v = bi_c.value\n",
    "        global_mean_v = global_mean_c.value\n",
    "        uc_v = uc_c.value\n",
    "        ic_v = ic_c.value\n",
    "        sc_v = sc_c.value\n",
    "        alpha_v = alpha_c.value\n",
    "        reg_v = reg_c.value\n",
    "        \n",
    "        for row in iterator:\n",
    "            uid = row[uc_v] #userid\n",
    "            iid = row[ic_v] #artistid\n",
    "            real_rating = row[sc_v] #playcount\n",
    "            error = real_rating - (global_mean_v + bu_v['bu'][uid] + bi_v['bi'][iid])\n",
    "            #update bias\n",
    "            bu_v['bu'][uid] += alpha_v * (error - reg_v * bu_v['bu'][uid])\n",
    "            bi_v['bi'][iid] += alpha_v * (error - reg_v * bi_v['bi'][iid])\n",
    "        return (bu_v, bi_v) #return a updated bias dictionary for user and artist\n",
    "        \n",
    "    for i in range(num_iter):\n",
    "        tmp_result = train_rdd.mapPartitions(__calculate) #apply calculate function to all train_rdd partitions\n",
    "        results = tmp_result.collect()\n",
    "        bu_result = {}\n",
    "        bi_result = {}\n",
    "        for result in results:\n",
    "            has_bu = 'bu' in result.keys()\n",
    "            has_bi = 'bi' in result.keys()\n",
    "            if has_bu: #put user bias in dictionary\n",
    "                if len(bu_result)==0:\n",
    "                    bu_result = result['bu']\n",
    "                else:\n",
    "                    for k, v in result['bu'].items():\n",
    "                        bu_result[k] += v\n",
    "            if has_bi: #put artist bias in dictionary\n",
    "                if len(bi_result)==0:\n",
    "                    bi_result = result['bi']\n",
    "                else:\n",
    "                    for k, v in result['bi'].items():\n",
    "                        bi_result[k] += v\n",
    "                        \n",
    "        # take bias value from each partition，and calculate mean(PSDG)     \n",
    "        for k, v in bu_result.items():\n",
    "            bu_result[k] = v / num_partition\n",
    "        for k, v in bi_result.items():\n",
    "            bi_result[k] = v / num_partition\n",
    "        bu['bu'] = bu_result\n",
    "        bi['bi'] = bi_result    \n",
    "        alpha *= rate_alpha #update learning rate\n",
    "    return bi_df, global_mean, bu, bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below, the function \"predict\" is used to predict the play count, and the performance of models can be evaluated using \"evaluate_rmse\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predict_df, global_mean, bu, bi, uc='user_id', ic='artist_id', prec='predict'):\n",
    "    '''\n",
    "    Predict the numebr of play count by adding user bias and item bias to mean play count\n",
    "    '''\n",
    "    global_mean_c = spark.sparkContext.broadcast(global_mean)\n",
    "    bu_c = spark.sparkContext.broadcast(bu)\n",
    "    bi_c = spark.sparkContext.broadcast(bi)\n",
    "        \n",
    "    @udf(FloatType())\n",
    "    def predict_count(uid, iid):\n",
    "        global_mean_v = global_mean_c.value\n",
    "        bu_v = bu_c.value\n",
    "        bi_v = bi_c.value\n",
    "        return global_mean_v + bu_v['bu'][uid] + bi_v['bi'][iid]\n",
    "        \n",
    "    result_df = predict_df.withColumn(prec, predict_count(predict_df[uc], predict_df[ic]))\n",
    "    return result_df   \n",
    "\n",
    "\n",
    "def evaluate_rmse(df, sc='play_count', prec ='predict'):\n",
    "    '''\n",
    "    Evaluate the model perfomance using rmse\n",
    "    '''\n",
    "    @udf(FloatType())\n",
    "    def rmse(score, predict):\n",
    "        return math.pow(score-predict, 2) #(real_playcount-predicted_playcount)^2\n",
    "    rmse_df = df.withColumn('RMSE2', rmse(df[sc], df[prec])) #rmse   \n",
    "    RMSE2_sum, count = rmse_df.rdd.map(lambda x: (1, (x['RMSE2'], 1))) \\\n",
    "        .reduceByKey(lambda x1, x2: (x1[0]+x2[0], x1[1]+x2[1])) \\\n",
    "        .collect()[0][1] #sum the RMSE2_sum\n",
    "    return math.sqrt(RMSE2_sum/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A user ID and the number of artists to recommend are inputs of the \"recommend\" function below. It starts with getting the top n users that have similar music tastes with the input user. Then fits their user vector (including artists they listen to, corresponding play count, and score) into the SGD method to obtain optimize biases for users and artists. A large number of data in one partition would affect the model performance. Therefore, 4000 data are computed in a partition, and the number of iteration is 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(uid, n):\n",
    "    '''\n",
    "    @param uid: the user id to recommend \n",
    "    @param n: numeber of artist to recommend\n",
    "    '''\n",
    "    sdg_sim = recommend_model.get_sim_n(uid, n) #get the most similiary n users\n",
    "    sdg_df = sdg_sim.join(recommend_train_df, ['user_id'], 'left').select('user_id', 'artist_id', 'play_count', 'score')\n",
    "    count = sdg_df.count()\n",
    "    bi_df, global_mean, bu, bi = fit_sgd(sdg_df, num_partition=count // 4000 + 1, num_iter=500) #train sgd model\n",
    "    \n",
    "    #predict and get rmse score of model\n",
    "    test = sdg_df.limit(1000)\n",
    "    pre_df = predict(test, global_mean, bu, bi)\n",
    "    rmse = evaluate_rmse(pre_df)\n",
    "    \n",
    "    #predict top n artist for user\n",
    "    result_pre_df = bi_df.withColumn('user_id', lit(uid)).select('user_id', 'artist_id')\n",
    "    result_pre_df = result_pre_df.select(result_pre_df['user_id'].cast('int'), 'artist_id')\n",
    "    sgd_result = predict(result_pre_df, global_mean, bu, bi).orderBy(desc('predict')).limit(n) \n",
    "            \n",
    "    return sgd_result, rmse  \n",
    "\n",
    "#recommand 5 artist to user \"1000002\"\n",
    "result, rmse_score = recommend('1000002', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read artist_data.txt dataset \n",
    "artist_schema = StructType([\n",
    "    StructField(\"artistId\", LongType(), True),    \n",
    "    StructField(\"artistName\", StringType(), True)])\n",
    "artist_data_df = spark.read.csv('./profiledata_06-May-2005/artist_data.txt', sep = \"\\t\", header=False, schema = artist_schema)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A music recommender built by SGD is now down. Next, I try to recommend 5 artists to user '1000002'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dream Theater\n",
      "Miles Davis\n",
      "Less Than Jake\n",
      "Free\n",
      "Badly Drawn Boy\n"
     ]
    }
   ],
   "source": [
    "n=5\n",
    "recommend_list = [row['artist_id'] for row in result.select('artist_id').collect()]\n",
    "#replace the artist id to the corresponding artist name.\n",
    "for i in range(n):\n",
    "    artist = artist_data_df.rdd.filter(lambda x:x[0] == recommend_list[i]).collect()[0][1] \n",
    "    print(artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the performance of the SGD model is evaluated by the same criteria as for ALS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9360720178682889"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "In conclusion, collaborative filtering implements ALS and SGD are used in this project. The performance of models is evaluated by RMSE. The RMSE value of these two models is similar. However, the implementation time of the SGD model is much longer than ALS. Therefore, to save the cost of the implementation, ALS does a better job. Overall, I think that the results of this project can be improved and this topic can be developed more in future research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "Rajendra Sahu, Manoj Kumar Dash, Anil Kumar. Applying Predictive Analytics Within the Service Sector. P189. 2017.\n",
    "\n",
    "Sarwar, Badrul, et al. \"Item-based collaborative filtering recommendation algorithms.\" Proceedings of the 10th international conference on World Wide Web. 2001.\n",
    "\n",
    "Zinkevich, Martin, et al. \"Parallelized stochastic gradient descent.\" Advances in neural information processing systems. 2010.\n",
    "\n",
    "Zhuang, Yong, et al. \"A fast parallel SGD for matrix factorization in shared memory systems.\" Proceedings of the 7th ACM conference on Recommender systems. 2013.\n",
    "\n",
    "Manuel Pozo, Raja Chiky. An implementation of a Distributed Stochastic Gradient Descent for Recommender Systems based on Map-Reduce, Oct 2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
